{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the tutorial, we run two ontology based methods to produce vector representations of biological entities: Onto2Vec and OPA2Vec.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onto2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onto2vec produces vectory representations based on the logical axioms of an ontology and the known associations between ontology classes and biological entities. In the case study below, we use Onto2vec to produce vector representations of proteins based on their GO annotations and the GO logical axioms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\n",
      "\t\t*********** Onto2Vec Running ... ***********\n",
      "\n",
      "\n",
      "\t\t1.Reasoning over ontology ...\n",
      "\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "Loading of Axioms ...\n",
      "Loading ...\n",
      "    1%\n",
      "    2%\n",
      "    3%\n",
      "    4%\n",
      "    6%\n",
      "    7%\n",
      "    9%\n",
      "    10%\n",
      "    12%\n",
      "    14%\n",
      "    16%\n",
      "    18%\n",
      "    19%\n",
      "    21%\n",
      "    23%\n",
      "    25%\n",
      "    27%\n",
      "    28%\n",
      "    30%\n",
      "    32%\n",
      "    34%\n",
      "    36%\n",
      "    37%\n",
      "    39%\n",
      "    42%\n",
      "    44%\n",
      "    46%\n",
      "    48%\n",
      "    50%\n",
      "    52%\n",
      "    54%\n",
      "    56%\n",
      "    57%\n",
      "    59%\n",
      "    61%\n",
      "    63%\n",
      "    65%\n",
      "    67%\n",
      "    69%\n",
      "    71%\n",
      "    74%\n",
      "    76%\n",
      "    78%\n",
      "    79%\n",
      "    80%\n",
      "    81%\n",
      "    82%\n",
      "    83%\n",
      "    84%\n",
      "    86%\n",
      "    87%\n",
      "    88%\n",
      "    89%\n",
      "    90%\n",
      "    91%\n",
      "    93%\n",
      "    94%\n",
      "    96%\n",
      "    97%\n",
      "    98%\n",
      "    ... finished\n",
      "    ... finished\n",
      "Property Saturation Initialization ...\n",
      "    ... finished\n",
      "Reflexive Property Computation ...\n",
      "    ... finished\n",
      "Object Property Hierarchy and Composition Computation ...\n",
      "    ... finished\n",
      "Context Initialization ...\n",
      "    ... finished\n",
      "Consistency Checking ...\n",
      "    100%\n",
      "    ... finished\n",
      "Class Taxonomy Computation ...\n",
      "    34%\n",
      "    74%\n",
      "    ... finished\n",
      "\n",
      "   ######################################################################\n",
      "\n",
      "\n",
      "\t\t2.Propagate associations through hierarchy ...\n",
      "\n",
      "\n",
      "   ######################################################################\n",
      "\n",
      "\n",
      "\t\t3.Corpus creation ...\n",
      "\n",
      "\n",
      "  ######################################################################\n",
      "\n",
      "\n",
      "\t\t4.Running word2Vec ... \n",
      "\n",
      "\n",
      "  ######################################################################\n",
      "\n",
      "\n",
      "\t\t5.Processing vectors ... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "org_id ='4932' #or 9606 for human data \n",
    "!python onto2vec/runOnto2Vec.py  -ontology data/go.owl -associations data/train/{org_id}.OPA_associations.txt -outfile data/{org_id}.onto2vec_vecs -entities data/train/{org_id}.protein_list.txt  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## OPA2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the ontology axioms and their entity associations, OPA2Vec also uses the ontology metadata and literature to represent biological entities. The code below runs OPA2Vec on GO and protein-GO associations to produce protein vector representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\n",
      "\t\t*********** OPA2Vec Running ... ***********\n",
      "\n",
      "\n",
      "\t\t1.Ontology Processing ...\n",
      "\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "Loading of Axioms ...\n",
      "Loading ...\n",
      "    1%\n",
      "    2%\n",
      "    3%\n",
      "    4%\n",
      "    5%\n",
      "    6%\n",
      "    7%\n",
      "    8%\n",
      "    9%\n",
      "    10%\n",
      "    11%\n",
      "    12%\n",
      "    13%\n",
      "    14%\n",
      "    15%\n",
      "    16%\n",
      "    18%\n",
      "    19%\n",
      "    20%\n",
      "    21%\n",
      "    23%\n",
      "    25%\n",
      "    26%\n",
      "    28%\n",
      "    29%\n",
      "    31%\n",
      "    33%\n",
      "    34%\n",
      "    36%\n",
      "    37%\n",
      "    38%\n",
      "    40%\n",
      "    42%\n",
      "    44%\n",
      "    45%\n",
      "    47%\n",
      "    49%\n",
      "    51%\n",
      "    53%\n",
      "    55%\n",
      "    57%\n",
      "    59%\n",
      "    61%\n",
      "    63%\n",
      "    64%\n",
      "    66%\n",
      "    67%\n",
      "    68%\n",
      "    69%\n",
      "    70%\n",
      "    71%\n",
      "    73%\n",
      "    74%\n",
      "    75%\n",
      "    76%\n",
      "    78%\n",
      "    80%\n",
      "    81%\n",
      "    83%\n",
      "    84%\n",
      "    86%\n",
      "    87%\n",
      "    89%\n",
      "    91%\n",
      "    92%\n",
      "    94%\n",
      "    95%\n",
      "    97%\n",
      "    99%\n",
      "    ... finished\n",
      "    ... finished\n",
      "Property Saturation Initialization ...\n",
      "    ... finished\n",
      "Reflexive Property Computation ...\n",
      "    ... finished\n",
      "Object Property Hierarchy and Composition Computation ...\n",
      "    ... finished\n",
      "Context Initialization ...\n",
      "    ... finished\n",
      "Consistency Checking ...\n",
      "    100%\n",
      "    ... finished\n",
      "Class Taxonomy Computation ...\n",
      "    5%\n",
      "    10%\n",
      "    18%\n",
      "    29%\n",
      "    40%\n",
      "    54%\n",
      "    71%\n",
      "    85%\n",
      "    ... finished\n",
      "\n",
      "   ######################################################################\n",
      "\n",
      "\n",
      "\t\t2.Metadata Extraction ...\n",
      "\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "\n",
      "   ######################################################################\n",
      "\n",
      "\n",
      "\t\t3.Propagate Associations through hierarchy ...\n",
      "\n",
      "\n",
      "   ######################################################################\n",
      "\n",
      "\n",
      "\t\t4.Corpus Creation ...\n",
      "\n",
      "\n",
      "  ######################################################################\n",
      "\n",
      "\n",
      "\t\t5.Running Word2Vec ... \n",
      "\n",
      "\n",
      "  ######################################################################\n",
      "\n",
      "\n",
      "\t\t6.Processing vectors ... \n",
      "\n",
      "\n",
      "\t\t*********** OPA2Vec Complete ***********\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python opa2vec/runOPA2Vec.py  -ontology data/go.owl -associations data/train/{org_id}.OPA_associations.txt -outfile data/{org_id}.opa2vec_vecs -entities data/train/{org_id}.protein_list.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map proteins to corresponding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "org_id = '9606' #org_id = '4932'\n",
    "onto2vec_map = {} \n",
    "opa2vec_map = {}\n",
    "with open (f'data/{org_id}.onto2vec_vecs','r') as f:\n",
    "       for line in f:\n",
    "           protein, vector=line.strip().split(\" \",maxsplit=1)\n",
    "           onto2vec_map [protein]=vector\n",
    "with open (f'data/{org_id}.opa2vec_vecs','r') as f:\n",
    "       for line in f:\n",
    "            protein, vector=line.strip().split(\" \",maxsplit=1)\n",
    "            opa2vec_map [protein]=vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate pair features for the training/validation/testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random \n",
    "data_type = ['train', 'valid', 'test']\n",
    "for i in data_type:\n",
    "        pair_data = []\n",
    "        feature_vecs =[]\n",
    "        label_map ={}\n",
    "        with open (f'data/{i}/{org_id}.protein.links.v11.0.txt','r') as f1:\n",
    "              for line in f1:\n",
    "                  prot1, prot2 = line.strip().split()\n",
    "                  pair_data.append((prot1,prot2))\n",
    "                  label_map[(prot1, prot2)] = 1\n",
    "        with open (f'data/{i}/{org_id}.negative_interactions.txt','r') as f2:\n",
    "             for line in f2:\n",
    "                  prot1, prot2 = line.strip().split()\n",
    "                  pair_data.append((prot1, prot2))\n",
    "                  label_map[(prot1, prot2)] = 0 \n",
    "        random.shuffle(pair_data)\n",
    "        with open (f'data/{i}/{org_id}.onto2vec_features','w') as f3:\n",
    "              with open (f'data/{i}/{org_id}.opa2vec_features', 'w') as f4:\n",
    "                   with open (f'data/{i}/{org_id}.labels','w') as f5:\n",
    "                        with open (f'data/{i}/{org_id}.pairs','w') as f6:\n",
    "                             for prot1, prot2 in pair_data:\n",
    "                                 if (prot1 in onto2vec_map and prot1 in opa2vec_map and prot2 in onto2vec_map and prot2 in opa2vec_map):\n",
    "                                   f6.write (f'{prot1} {prot2}\\n')\n",
    "                                   f5.write (f'{label_map[(prot1,prot2)]}\\n')\n",
    "                                   f4.write (f'{opa2vec_map[prot1]} {opa2vec_map[prot2]}\\n')\n",
    "                                   f3.write (f'{onto2vec_map[prot1]} {onto2vec_map[prot2]}\\n')   \n",
    "                                    "
   ]
  },
 {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Calculating cosine similarity to explore neighbors of each protein and finding most similar protein vectors. The interaction prediction is then performed based on similarity value based on the assumption that proteins with highly similar feature vectors are more like to interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cosine \n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "for prot1, prot2 in pair_data:\n",
    "    v1_onto = onto2vec_map[prot1]\n",
    "    v2_onto = onto2vec_map [prot2]\n",
    "    v1_opa = opa2vec_map [prot1]\n",
    "    v2_opa = opa2vec_map [prot2]\n",
    "    if (prot1 in onto2vec_map and prot1 in opa2vec_map and prot2 in onto2vec_map and prot2 in opa2vec_map):\n",
    "        cosine_onto = cosine(v1_onto, v2_onto)\n",
    "        cosine_opa = cosine (v1_opa, v2_opa)\n",
    "    with open (f'data/{i}/{org_id}.onto_sim','w') as onto_cos:\n",
    "         with open (f'data/{i}/{org_id}.opa_sim','w') as opa_cos:\n",
    "            onto_cos.write (f'{cosine_onto}\\n')\n",
    "                opa_cos.write (f'{cosine_onto}\\n')\n",
    "        \n",
    "        \n",
    "query =str(sys.argv[1])\n",
    "n = int (sys.argv[2])\n",
    "#query =\"A0A024RBG1\"\n",
    "#n=10\n",
    "vectors=numpy.loadtxt(\"data/{org_id}.opa2vec_vecs\");\n",
    "text_file=\"data/train/protein_list\"\n",
    "classfile=open (text_file)\n",
    "mylist=[]\n",
    "for linec in classfile:\n",
    "\tmystr=linec.strip()\n",
    "\tmylist.append(mystr)\n",
    "\n",
    "\n",
    "#3.Mapping Entities to Vectors\n",
    "vectors_map={}\n",
    "for i in range(0,len(mylist)):\n",
    "\tvectors_map[mylist[i]]=vectors[i,:]\n",
    "\t\n",
    "\n",
    "\n",
    "cosine_sim={}\n",
    "for x in range(0,len(mylist)):\n",
    "\tif (mylist[x]!=query): \t\n",
    "\t\tv1=vectors_map[mylist[x]]\n",
    "\t\tv2=vectors_map[query]\n",
    "\t\tvalue=cosine(v1,v2)\n",
    "\t\tcosine_sim[mylist[x]]=value\n",
    "classes = mylist\n",
    "#5.Retrieving neighbors \n",
    "sortedmap=sorted(cosine_sim,key=cosine_sim.get, reverse=True)\n",
    "iterator=islice(sortedmap,n)\n",
    "i =1\n",
    "for d in iterator:\n",
    "\tprint (str(i)+\". \"+ str(d) +\"\\t\"+str(cosine_sim[d])+\"\\n\")\n",
    "\ti +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import rankdata\n",
    "\n",
    "def load_test_data(data_file, classes):\n",
    "    data = []\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            it = line.strip().split()\n",
    "            id1 = f'http://{it[0]}'\n",
    "            id2 = f'http://{it[1]}'\n",
    "            data.append((id1, id2))\n",
    "    return data\n",
    "\n",
    "def compute_rank_roc(ranks, n_prots):\n",
    "    auc_x = list(ranks.keys())\n",
    "    auc_x.sort()\n",
    "    auc_y = []\n",
    "    tpr = 0\n",
    "    sum_rank = sum(ranks.values())\n",
    "    for x in auc_x:\n",
    "        tpr += ranks[x]\n",
    "        auc_y.append(tpr / sum_rank)\n",
    "    auc_x.append(n_prots)\n",
    "    auc_y.append(1)\n",
    "    auc = np.trapz(auc_y, auc_x) / n_prots\n",
    "    return auc\n",
    "\n",
    "\n",
    "\n",
    "# Load test data and compute ranks for each protein\n",
    "test_data = load_test_data(f'data/test/{org_id}.protein.links.v11.0.txt', classes)\n",
    "top1 = 0\n",
    "top10 = 0\n",
    "top100 = 0\n",
    "mean_rank = 0\n",
    "ftop1 = 0\n",
    "ftop10 = 0\n",
    "ftop100 = 0\n",
    "fmean_rank = 0\n",
    "labels = {}\n",
    "preds = {}\n",
    "ranks = {}\n",
    "franks = {}\n",
    "eval_data = test_data\n",
    "n = len(eval_data)\n",
    "for c, d in eval_data:\n",
    "    c, d = prot_dict[classes[c]], prot_dict[classes[d]]\n",
    "    labels = np.zeros((len(onto2vec_map), len(onto2vec_map)), dtype=np.int32)\n",
    "    preds = np.zeros((len(onto2vec_map), len(onto2vec_map)), dtype=np.float32)\n",
    "    labels[c, d] = 1\n",
    "    ec = onto2vec_map[c, :]\n",
    "    #er = rembeds[r, :]\n",
    "    #ec += er\n",
    "\n",
    "    # Compute distance\n",
    "    #dst = np.linalg.norm(prot_embeds - ec.reshape(1, -1), axis=1)\n",
    "    res = numpy.loadtxt('onto_cos.write')\n",
    "\n",
    "    preds[c, :] = res\n",
    "    index = rankdata(res, method='average')\n",
    "    rank = index[d]\n",
    "    if rank == 1:\n",
    "        top1 += 1\n",
    "    if rank <= 10:\n",
    "        top10 += 1\n",
    "    if rank <= 100:\n",
    "        top100 += 1\n",
    "    mean_rank += rank\n",
    "    if rank not in ranks:\n",
    "        ranks[rank] = 0\n",
    "    ranks[rank] += 1\n",
    "\n",
    "    # Filtered rank\n",
    "    index = rankdata((res * trlabels[c, :]), method='average')\n",
    "    rank = index[d]\n",
    "    if rank == 1:\n",
    "        ftop1 += 1\n",
    "    if rank <= 10:\n",
    "        ftop10 += 1\n",
    "    if rank <= 100:\n",
    "        ftop100 += 1\n",
    "    fmean_rank += rank\n",
    "\n",
    "    if rank not in franks:\n",
    "        franks[rank] = 0\n",
    "    franks[rank] += 1\n",
    "top1 /= n\n",
    "top10 /= n\n",
    "top100 /= n\n",
    "mean_rank /= n\n",
    "ftop1 /= n\n",
    "ftop10 /= n\n",
    "ftop100 /= n\n",
    "fmean_rank /= n\n",
    "\n",
    "rank_auc = compute_rank_roc(ranks, len(proteins))\n",
    "frank_auc = compute_rank_roc(franks, len(proteins))\n",
    "\n",
    "print(f'{top10:.2f} {top100:.2f} {mean_rank:.2f} {rank_auc:.2f}')\n",
    "print(f'{ftop10:.2f} {ftop100:.2f} {fmean_rank:.2f} {frank_auc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from random import randint \n",
    "import numpy as np\n",
    "import time\n",
    "import os \n",
    "import sys \n",
    "import numpy\n",
    "import sklearn \n",
    "\n",
    "#Hyperparameters\n",
    "num_epochs = 100\n",
    "num_classes = 2\n",
    "batch_size = 50\n",
    "learning_rate = 0.0001\n",
    "\n",
    "\n",
    "#Load dataset \n",
    "X_train_1= numpy.loadtxt(\"data/train/{org_id}.embeddings_1\")\n",
    "X_train_2= numpy.loadtxt(\"data/train/{org_id}.embeddings_2\")\n",
    "y_train= numpy.loadtxt(\"data/train/{org_id}.labels\")\n",
    "\n",
    "X_test_1= numpy.loadtxt(\"data/test/{org_id}.embeddings_1\")\n",
    "X_test_2= numpy.loadtxt(\"data/test/{org_id}.embeddings_2\")\n",
    "y_test= numpy.loadtxt(\"data/test/{org_id}.labels\")\n",
    "\n",
    "#transform to torch\n",
    "train_x1= torch.from_numpy(X_train_1).float()\n",
    "train_x2= torch.from_numpy(X_train_2).float()\n",
    "train_x = [train_x1, train_x2]\n",
    "train_label= torch.from_numpy(y_train).long()\n",
    "\n",
    "\n",
    "test_x1 = torch.from_numpy(X_test_1).float()\n",
    "test_x2 = torch.from_numpy(X_test_2).float()\n",
    "test_x=[test_x1, test_x2]\n",
    "test_label= torch.from_numpy(y_test).long()\n",
    "\n",
    "\n",
    "train_data = []\n",
    "train_data.append([train_x, train_label])\n",
    "\n",
    "test_data = []\n",
    "test_data.append([test_x,test_label])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#Define Network \n",
    "class Net (nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Net, self).__init__()\n",
    "\t\tself.layer1 = nn.Sequential(\n",
    "\t\t\tnn.Linear (200, 600),\n",
    "\t\t\tnn.ReLU())\n",
    "\t\tself.layer2 = nn.Sequential (\n",
    "\t\t\tnn.Linear (600,400),\n",
    "\t\t\tnn.ReLU())\n",
    "\t\tself.layer3 = nn.Sequential(\n",
    "\t\t\tnn.Linear (400, 200),\n",
    "\t\t\tnn.ReLU())\n",
    "\t\tself.drop_out = nn.Dropout()\n",
    "\t\tself.dis = nn.Linear (200,2)\n",
    "\n",
    "\t\t\t\t\n",
    "\tdef forward (self, data):\n",
    "\t\tres = []\n",
    "\t\tfor i in range(2):\n",
    "\t\t\tx = data[i]\n",
    "\t\t\tout = self.layer1(x)\n",
    "\t\t\tout = self.layer2(out)\n",
    "\t\t\tout = self.layer3(out)\n",
    "\t\t\tout = self.drop_out(out)\n",
    "\t\t\t#out = out.reshape(out.size(0),-1)\n",
    "\t\t\tres.append(out)\n",
    "\t\toutput = torch.abs(res[1] - res[0])\n",
    "\t\t#output = torch.mm(res[1] , res[0])\t\t\n",
    "\t\toutput = self.dis(output)\n",
    "\t\treturn output\n",
    "\n",
    "#Create network \n",
    "network = Net()\n",
    "\n",
    "# Use Cross Entropy for back propagation \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam (network.parameters(),lr=learning_rate)\n",
    "\n",
    "# Train the model \n",
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "for epoch in range (num_epochs):\n",
    "\tfor i, (train_x, train_label) in enumerate (train_loader):\n",
    "\t\t# Get data\n",
    "\t\tinputs = train_x\n",
    "\t\tlabels = train_label\n",
    "\n",
    "\t\t# Run the forward pass\n",
    "\t\toutputs = network (inputs)\n",
    "\t\toutputs=outputs.reshape(-1,2)\n",
    "\t\tlabels=labels.reshape(-1)\t\t\t\t\n",
    "\t\t#print (outputs.size())\n",
    "\t\t#print (labels.size())\n",
    "\t\tloss = criterion (outputs, labels)\n",
    "\t\tloss_list.append(loss.item())\n",
    "\t\n",
    "\t\t# Back propagation and optimization\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\t# Get prediction\n",
    "\t\ttotal = labels.size(0)\n",
    "\t\t_,predicted = torch.max(outputs.data,1)\n",
    "\t\tcorrect = (predicted == labels).sum().item()\n",
    "\t\tacc_list.append (correct/total)\n",
    "\t\t\n",
    "\t\t#if (i + 1) % 100 == 0:\n",
    "\t\tprint ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item(), (correct/total)*100))\n",
    "\n",
    "\n",
    "# Test the model \n",
    "network.eval()\n",
    "with torch.no_grad():\n",
    "\tcorrect = 0\n",
    "\ttotal = 0\n",
    "\tfor test_x,test_label in  test_loader:\n",
    "\t\toutputs = network (test_x)\n",
    "\t\tlabels = test_label\n",
    "\t\toutputs=outputs.reshape(-1,2)\t\t\n",
    "\t\tarray = outputs.data.cpu().numpy()\n",
    "\t\tnumpy.savetxt('output.csv',array)\n",
    "\t\tlabels=labels.reshape(-1)\t\n",
    "\t\t_, predicted = torch.max(outputs.data,1)\n",
    "\t\ttotal += labels.size(0)\n",
    "\t\tcorrect += (predicted == labels).sum().item()\n",
    "\t#print ('Accuracy of model on test dataset is: {} %'.format((correct / total) *100))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
