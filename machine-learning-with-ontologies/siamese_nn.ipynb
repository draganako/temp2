{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese Neural Network for predicting PPIs from function annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import click as ck\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Concatenate, Dot, Activation\n",
    ")\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import constraints\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "import math\n",
    "from scipy.stats import rankdata\n",
    "from elembeddings.utils import Ontology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of proteins in training:  16371\n",
      "Training interactions:  538286\n",
      "Validation interactions:  133618\n",
      "Testing interactions:  167068\n"
     ]
    }
   ],
   "source": [
    "org_id = '9606'\n",
    "\n",
    "def load_train_data(data_file):\n",
    "    data = []\n",
    "    proteins = {}\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            it = line.strip().split()\n",
    "            id1 = it[0]\n",
    "            id2 = it[1]\n",
    "            if id1 not in proteins:\n",
    "                proteins[id1] = len(proteins)\n",
    "            if id2 not in proteins:\n",
    "                proteins[id2] = len(proteins)\n",
    "            data.append((proteins[id1], proteins[id2]))\n",
    "    return data, proteins\n",
    "\n",
    "def load_test_data(data_file, proteins):\n",
    "    data = []\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            it = line.strip().split()\n",
    "            id1 = it[0]\n",
    "            id2 = it[1]\n",
    "            if id1 not in proteins or id2 not in proteins:\n",
    "                continue\n",
    "            data.append((proteins[id1], proteins[id2]))\n",
    "    return data\n",
    "\n",
    "train_data, proteins = load_train_data(f'data/train/{org_id}.protein.links.v11.0.txt')\n",
    "valid_data = load_test_data(f'data/valid/{org_id}.protein.links.v11.0.txt', proteins)\n",
    "test_data = load_test_data(f'data/test/{org_id}.protein.links.v11.0.txt', proteins)\n",
    "print('Number of proteins in training: ', len(proteins))\n",
    "print('Training interactions: ', len(train_data))\n",
    "print('Validation interactions: ', len(valid_data))\n",
    "print('Testing interactions: ', len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load functional annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded annotations for 14888 proteins\n",
      "Total number of distinct functions 15494\n"
     ]
    }
   ],
   "source": [
    "def load_annotations(data_file, proteins, propagate=False):\n",
    "    go = Ontology('data/go.obo')\n",
    "    annots = {}\n",
    "    functions = set()\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            it = line.strip().split('\\t')\n",
    "            if it[0] not in proteins:\n",
    "                continue\n",
    "            p_id = proteins[it[0]]\n",
    "            if p_id not in annots:\n",
    "                annots[p_id] = set()\n",
    "            annots[p_id].add(it[1])\n",
    "            functions.add(it[1])\n",
    "            if propagate and go.has_term(it[1]):\n",
    "                annots[p_id] |= go.get_anchestors(it[1])\n",
    "                functions |= go.get_anchestors(it[1])\n",
    "    functions = list(functions)\n",
    "    return annots, functions\n",
    "\n",
    "# Run this function with propagate=False to use annotations without propagation with ontology structure\n",
    "annotations, functions = load_annotations(f'data/train/{org_id}.annotation.txt', proteins, propagate=False)\n",
    "print('Loaded annotations for', len(annotations), 'proteins')\n",
    "print('Total number of distinct functions', len(functions))\n",
    "functions_ix = {k:i for i, k in enumerate(functions)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator object for feeding neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(object):\n",
    "\n",
    "    def __init__(self, data, proteins, annotations, train_pairs, functions_ix, batch_size=128, steps=100):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = steps\n",
    "        self.start = 0\n",
    "        self.functions_ix = functions_ix\n",
    "        self.input_length = len(functions_ix)\n",
    "        self.train_pairs = train_pairs\n",
    "        self.proteins = proteins\n",
    "        self.annotations = annotations\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def reset(self):\n",
    "        self.start = 0\n",
    "\n",
    "    def next(self):\n",
    "        if self.start < self.steps:\n",
    "            batch_pos = self.data[self.start * self.batch_size: (self.start + 1) * self.batch_size]\n",
    "            batch_neg = []\n",
    "            for pr1, pr2 in batch_pos:\n",
    "                flag = np.random.choice([True, False])\n",
    "                while True:\n",
    "                    neg = np.random.randint(0, len(self.proteins))\n",
    "                    if flag:\n",
    "                        if (pr1, neg) not in train_pairs:\n",
    "                            batch_neg.append((pr1, neg))\n",
    "                            break\n",
    "                    else:\n",
    "                        if (neg, pr2) not in train_pairs:\n",
    "                            batch_neg.append((neg, pr2))\n",
    "                            break\n",
    "            batch_data = np.array(batch_pos + batch_neg)\n",
    "            labels = np.array([1] * len(batch_pos) + [0] * len(batch_neg))\n",
    "            index = np.arange(len(batch_data))\n",
    "            np.random.shuffle(index)\n",
    "            batch_data = batch_data[index]\n",
    "            labels = labels[index]\n",
    "            p1 = np.zeros((len(batch_data), self.input_length), dtype=np.float32)\n",
    "            p2 = np.zeros((len(batch_data), self.input_length), dtype=np.float32)\n",
    "            for i in range(len(batch_data)):\n",
    "                if batch_data[i, 0] in self.annotations:\n",
    "                    for go_id in self.annotations[batch_data[i, 0]]:\n",
    "                        p1[i, self.functions_ix[go_id]] = 1.0\n",
    "                if batch_data[i, 1] in self.annotations:\n",
    "                    for go_id in self.annotations[batch_data[i, 1]]:\n",
    "                        p2[i, self.functions_ix[go_id]] = 1.0\n",
    "            self.start += 1\n",
    "            return ([p1, p2], labels)\n",
    "        else:\n",
    "            self.reset()\n",
    "train_pairs = set(train_data)\n",
    "batch_size = 128\n",
    "train_steps = int(math.ceil(len(train_data) / batch_size))\n",
    "train_generator = Generator(\n",
    "    train_data, proteins, annotations, train_pairs, functions_ix, batch_size=batch_size, steps=train_steps)\n",
    "valid_steps = int(math.ceil(len(valid_data) / batch_size))\n",
    "valid_generator = Generator(\n",
    "    valid_data, proteins, annotations, train_pairs, functions_ix, batch_size=batch_size, steps=valid_steps)\n",
    "test_steps = int(math.ceil(len(test_data) / batch_size))\n",
    "test_generator = Generator(\n",
    "    test_data, proteins, annotations, train_pairs, functions_ix, batch_size=batch_size, steps=test_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 15494)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 15494)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 256)          16523008    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1)            0           sequential[1][0]                 \n",
      "                                                                 sequential[2][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 1)            0           dot[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 16,523,008\n",
      "Trainable params: 16,523,008\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_model = Sequential()\n",
    "feature_model.add(Dense(1024, input_shape=(len(functions),), activation='relu'))\n",
    "feature_model.add(Dense(512, activation='relu'))\n",
    "feature_model.add(Dense(256, activation='relu'))\n",
    "\n",
    "input1 = Input(shape=(len(functions),))\n",
    "input2 = Input(shape=(len(functions),))\n",
    "feature1 = feature_model(input1)\n",
    "feature2 = feature_model(input2)\n",
    "net = Dot(axes=1)([feature1, feature2])\n",
    "net = Activation('sigmoid')(net)\n",
    "model = Model(inputs=[input1, input2], outputs=net)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "4206/4206 [==============================] - 158s 38ms/step - loss: 0.5064 - val_loss: 0.4872\n",
      "Epoch 2/12\n",
      "4206/4206 [==============================] - 158s 38ms/step - loss: 0.4812 - val_loss: 0.4778\n",
      "Epoch 3/12\n",
      "4206/4206 [==============================] - 152s 36ms/step - loss: 0.4747 - val_loss: 0.4736\n",
      "Epoch 4/12\n",
      "4206/4206 [==============================] - 157s 37ms/step - loss: 0.4715 - val_loss: 0.4699\n",
      "Epoch 5/12\n",
      "4206/4206 [==============================] - 155s 37ms/step - loss: 0.4690 - val_loss: 0.4695\n",
      "Epoch 6/12\n",
      "4206/4206 [==============================] - 153s 36ms/step - loss: 0.4668 - val_loss: 0.4675\n",
      "Epoch 7/12\n",
      "4206/4206 [==============================] - 151s 36ms/step - loss: 0.4653 - val_loss: 0.4676\n",
      "Epoch 8/12\n",
      "4206/4206 [==============================] - 151s 36ms/step - loss: 0.4647 - val_loss: 0.4666\n",
      "Epoch 9/12\n",
      "4206/4206 [==============================] - 150s 36ms/step - loss: 0.4629 - val_loss: 0.4644\n",
      "Epoch 10/12\n",
      "4206/4206 [==============================] - 153s 36ms/step - loss: 0.4623 - val_loss: 0.4637\n",
      "Epoch 11/12\n",
      "4206/4206 [==============================] - 151s 36ms/step - loss: 0.4611 - val_loss: 0.4613\n",
      "Epoch 12/12\n",
      "4206/4206 [==============================] - 150s 36ms/step - loss: 0.4597 - val_loss: 0.4620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7b9699f048>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 12\n",
    "earlystopper = EarlyStopping(patience=3)\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=valid_steps,\n",
    "    callbacks=[earlystopper,])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1306/1306 [==============================] - 27s 20ms/step\n",
      "Test loss: 0.4613165470497025\n"
     ]
    }
   ],
   "source": [
    "test_loss = model.evaluate_generator(test_generator, steps=test_steps, verbose=1)\n",
    "print('Test loss:', test_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get prediction scores for all pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of test proteins: 16371\n",
      "2093826/2093826 [==============================] - 21119s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "print('Total number of test proteins:', len(proteins))\n",
    "all_pairs = []\n",
    "for i in range(len(proteins)):\n",
    "    for j in range(len(proteins)):\n",
    "        all_pairs.append((i, j))\n",
    "\n",
    "batch_size = 128\n",
    "class SimpleGenerator(object):\n",
    "\n",
    "    def __init__(self, data, annotations, functions_ix, batch_size=128, steps=100):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = steps\n",
    "        self.start = 0\n",
    "        self.functions_ix = functions_ix\n",
    "        self.input_length = len(functions_ix)\n",
    "        self.annotations = annotations\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def reset(self):\n",
    "        self.start = 0\n",
    "\n",
    "    def next(self):\n",
    "        if self.start < self.steps:\n",
    "            batch_pairs = self.data[self.start * self.batch_size: (self.start + 1) * self.batch_size]\n",
    "            p1 = np.zeros((len(batch_pairs), self.input_length), dtype=np.float32)\n",
    "            p2 = np.zeros((len(batch_pairs), self.input_length), dtype=np.float32)\n",
    "            for i in range(len(batch_pairs)):\n",
    "                if batch_pairs[i][0] in self.annotations:\n",
    "                    for go_id in self.annotations[batch_pairs[i][0]]:\n",
    "                        p1[i, self.functions_ix[go_id]] = 1.0\n",
    "                if batch_pairs[i][1] in self.annotations:\n",
    "                    for go_id in self.annotations[batch_pairs[i][1]]:\n",
    "                        p2[i, self.functions_ix[go_id]] = 1.0\n",
    "            labels = np.zeros((len(batch_pairs), 1), dtype=np.float32)\n",
    "            self.start += 1\n",
    "            return ([p1, p2], labels)\n",
    "        else:\n",
    "            self.reset()\n",
    "\n",
    "all_steps = int(math.ceil(len(all_pairs) / batch_size))\n",
    "all_generator = SimpleGenerator(\n",
    "    all_pairs, annotations, functions_ix,\n",
    "    batch_size=batch_size, steps=all_steps)\n",
    "predictions = model.predict_generator(all_generator, steps=all_steps, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.05 0.41 1881.10 0.90\n",
      "0.15 0.64 1808.77 0.89\n"
     ]
    }
   ],
   "source": [
    "def compute_rank_roc(ranks, n_prots):\n",
    "    auc_x = list(ranks.keys())\n",
    "    auc_x.sort()\n",
    "    auc_y = []\n",
    "    tpr = 0\n",
    "    sum_rank = sum(ranks.values())\n",
    "    for x in auc_x:\n",
    "        tpr += ranks[x]\n",
    "        auc_y.append(tpr / sum_rank)\n",
    "    auc_x.append(n_prots)\n",
    "    auc_y.append(1)\n",
    "    auc = np.trapz(auc_y, auc_x) / n_prots\n",
    "    return auc\n",
    "\n",
    "\n",
    "sim = predictions.reshape(len(proteins), len(proteins))\n",
    "\n",
    "trlabels = np.ones((len(proteins), len(proteins)), dtype=np.int32)\n",
    "for c, d in train_data:\n",
    "    trlabels[c, d] = 0\n",
    "for c, d in valid_data:\n",
    "    trlabels[c, d] = 0\n",
    "\n",
    "top10 = 0\n",
    "top100 = 0\n",
    "mean_rank = 0\n",
    "ftop10 = 0\n",
    "ftop100 = 0\n",
    "fmean_rank = 0\n",
    "n = len(test_data)\n",
    "labels = np.zeros((len(proteins), len(proteins)), dtype=np.int32) \n",
    "ranks = {}\n",
    "franks = {}\n",
    "with ck.progressbar(test_data) as prog_data:\n",
    "    for c, d in prog_data:\n",
    "        labels[c, d] = 1\n",
    "        index = rankdata(-sim[c, :], method='average')\n",
    "        rank = index[d]\n",
    "        if rank <= 10:\n",
    "            top10 += 1\n",
    "        if rank <= 100:\n",
    "            top100 += 1\n",
    "        mean_rank += rank\n",
    "        if rank not in ranks:\n",
    "            ranks[rank] = 0\n",
    "        ranks[rank] += 1\n",
    "\n",
    "        # Filtered rank\n",
    "        fil = sim[c, :] * (labels[c, :] | trlabels[c, :])\n",
    "        index = rankdata(-fil, method='average')\n",
    "        rank = index[d]\n",
    "        if rank <= 10:\n",
    "            ftop10 += 1\n",
    "        if rank <= 100:\n",
    "            ftop100 += 1\n",
    "        fmean_rank += rank\n",
    "        if rank not in franks:\n",
    "            franks[rank] = 0\n",
    "        franks[rank] += 1\n",
    "\n",
    "    print()\n",
    "    top10 /= n\n",
    "    top100 /= n\n",
    "    mean_rank /= n\n",
    "    ftop10 /= n\n",
    "    ftop100 /= n\n",
    "    fmean_rank /= n\n",
    "\n",
    "    rank_auc = compute_rank_roc(ranks, len(proteins))\n",
    "    frank_auc = compute_rank_roc(franks, len(proteins))\n",
    "    print(f'{top10:.2f} {top100:.2f} {mean_rank:.2f} {rank_auc:.2f}')\n",
    "    print(f'{ftop10:.2f} {ftop100:.2f} {fmean_rank:.2f} {frank_auc:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
